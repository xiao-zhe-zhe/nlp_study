"""机器学习的底层目的及适用场景：
从大量已有数据中找出x（输入）与y（输出）的复杂规律，这个规律常表现为数学函数公式
再将这个规律运用于新输入，得出新输出
不同的机器学习算法决定计算去寻找复杂规律的方式
数据量太少，复杂规律会出现不明确的问题，可能存在多个可行规律，但无法确定唯一一个
将x（输入）如何数值化是重要问题，尽量不要损失太多信息
机器学习适合规律模糊难以用语言形容的情况，不适用规律明确"""

"""机器学习分类：
监督学习（标注信息，标注y目标输出，有明确结果要求）：e.g.机器翻译、图像识别
无监督学习（没有标注信息，没有标注y目标输出，只有x输入，分析x本身的规律）：e.g.聚类分析、高维向量降维
自监督学习（需有信息标注，但标注来源于x输入）
#机器学习的本质：从数据中寻找规律，用于预测未知样本；学习就是寻找模型中的参数，即权重矩阵W
#深度学习是机器学习的一种方法，是先选择模型公式，再调整参数“先猜后调”；但其他机器学习方法，如决策树，其训练思路不一样
"""

"""
训练集：用于模型训练，寻找规律
验证集：验证利用训练集所得模型是否过拟合  
测试集：用于最终评估算法模型效果，测试集和验证集在实际工作中区别不大
K fold交叉验证：（为了避免训练集数据简单，验证集过于困难）k次切分初始数据集，切分出不同的训练集和验证集，将k次训练结果取平均，得到最终结果"""

"""
过拟合：训练集学得过好，可能学了很多噪点，但在训练集上表现不好，模型失去泛化能力
欠拟合：训练集的东西都没学好
模型评价指标多样，依情况而定"""

"""
深度学习的大致定义：
1.模型随机初始化：随机选择一个模型函数，如（transformer、卷积神经网络、循环神经网络）；模型中有可调整的参数，随机确定一个参数
    #模型结构设定，会影响模型训练效率
    #随机初始化优化方法：在随机初始化之前，做一个预训练模型，可以提高参数初始化设置的效率
2.计算loss损失值：通过损失函数计算预测值与真实值之间的误差
    #损失函数优化，选择合理的损失函数，也可以提高模型训练效率
3.反向传播：调整模型参数，如（梯度下降法，确定模型参数调整的方向）
4.制定参数调整幅度策略：通过优化器、学习率
    #优化参数调整策略可以提高模型训练效率
5.依次迭代（Iteration）：重复以上过程，反复调整参数
6.最终结果：预测值与真实值无差距，loss=0，大多数模型无法达到0，loss函数必须≥0
    当loss函数不再具有下降趋势时，模型收敛"""

"""神经网络不同模型的本质区别：
神经网络模型中的隐藏层（在输入层和输出层之间的，也称为中间层）可以有不同结构（e.g.RNN CNN DNN LSTM Transformer等），本质是不同运算公式、不同逻辑
"""

"""导数与梯度：
导数：函数曲线上的斜率，该点的变化率
梯度：高维图像上的导数
#导数与梯度可以帮助判断损失函数的调整方向，通过对损失函数求导求其最小值
#梯度下降的目的：找到损失函数最小值
#一般希望损失函数本身的最小值为0
#损失函数本身的最小值（loss值）与模型的效果\训练的准确率从数值上无关，它只是损失函数既定的一个特殊值
#loss函数的最低点不一定是模型效果最佳点，可能是过拟合点
"""

"""优化器：
在损失值最小化的基础上，确定模型中权重矩阵W调整的幅度
"""

"""
Mini Batch：
把训练集分为多个部分，其中一个为一个batch，batch size的大小是一个可以调节的超参数
    #分batch可以减少训练内存，但可能会出现每个batch训练结果方向不同，最终整个训练集无解的情况
    #在训练过程中，每个batch会出现自己的梯度，以及自己的参数训练结构，最终整个训练集的结构由多个batch结果求平均
epoch：遍历完所有样本为一个epoch
模型多轮训练：“多轮训练”（通常称为多个Epoch）是指模型在整个训练数据集上完成多次完整的迭代过程。完成一个Epoch后，先打乱每个batch的顺序，再进行一轮训练
##概念区别：（来源：文心一言）
Epoch（轮次）：模型完整遍历一次整个训练数据集的过程。例如，若数据集有10,000张图片，batch size=100，则1个Epoch包含100次迭代（Iteration）
Iteration（迭代）：每次用1个batch的数据更新模型参数。1个Epoch包含多个Iteration
Batch Size：每次迭代使用的样本数量。较大的Batch Size可能加速训练，但需要更多内存
"""
